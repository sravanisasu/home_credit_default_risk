import mlflow
import numpy as np
import json
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import mlflow
from mlflow.models.signature import infer_signature
from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, MinMaxScaler
from sklearn.impute import SimpleImputer
from mlflow.models.signature import infer_signature

mlflow.set_registry_uri("databricks-uc")

model = mlflow.pyfunc.load_model(f'models:/cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.lightgbm/5')
loaded_le_flag_own_car = mlflow.sklearn.load_model(f'models:/cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.label_encoder_flag_own_car/5')
loaded_le_flag_own_realty = mlflow.sklearn.load_model(f'models:/cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.label_encoder_flag_own_realty/5')
loaded_le_name_contract_type = mlflow.sklearn.load_model(f'models:/cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.label_encoder_name_contract_type/5')
with open("/Workspace/Repos/ahstaff458@aia.com/atc_databricks_assessment/application_train_features.txt", 'r') as f:
    features_list = f.read().splitlines()
imputer_model_name = f'cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.imputer'
imputer = mlflow.sklearn.load_model(f'models:/{imputer_model_name}/5')
scaler_model_name = f'cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.scaler'
scaler = mlflow.sklearn.load_model(f'models:/{scaler_model_name}/5')


class CustomModel(mlflow.pyfunc.PythonModel):
    def __init__(self, model,loaded_le_flag_own_car,loaded_le_flag_own_realty,loaded_le_name_contract_type,features_list,imputer,scaler):
        # Load Models
        self.model = model
        self.loaded_le_flag_own_car = loaded_le_flag_own_car
        self.loaded_le_flag_own_realty = loaded_le_flag_own_realty
        self.loaded_le_name_contract_type = loaded_le_name_contract_type
        self.features_list = features_list
        self.imputer = imputer
        self.scaler = scaler 

    def load_context(self, context):
        self.context = context                       

    def pre_process(self, model_input):
        # Label Encode categorical variables
        model_input['FLAG_OWN_CAR'] = self.loaded_le_flag_own_car.transform(model_input['FLAG_OWN_CAR'])
        model_input['FLAG_OWN_REALTY'] = self.loaded_le_flag_own_realty.transform(model_input['FLAG_OWN_REALTY'])
        model_input['NAME_CONTRACT_TYPE'] = self.loaded_le_name_contract_type.transform(model_input['NAME_CONTRACT_TYPE'])
                
        # One hot Encode categorical variables
        model_input = pd.get_dummies(model_input)
        features_list = [feature for feature in self.features_list if feature != 'TARGET']
        model_input = model_input.reindex(columns=features_list, fill_value=0)

        # Preproces
        model_input['DAYS_EMPLOYED_ANOM'] = model_input["DAYS_EMPLOYED"] == 365243
        model_input["DAYS_EMPLOYED"].replace({365243: np.nan}, inplace = True)
        model_input['DAYS_BIRTH'] = abs(model_input['DAYS_BIRTH'])
        model_input = self.imputer.transform(model_input)
        model_input = self.scaler.transform(model_input)
        
        return model_input

    def predict(self, context, model_input:pd.DataFrame):
        # if isinstance(model_input, dict) and "columns" in model_input and "data" in model_input:
        #     model_input = pd.DataFrame(model_input["data"], columns=model_input["columns"])
        # elif not isinstance(model_input, pd.DataFrame):
        #     raise ValueError("Input must be a pandas DataFrame or a dictionary with 'columns' and 'data' keys.")

        # Add preprocessing step
        model_input = pd.DataFrame(model_input)
        model_input = self.pre_process(model_input)
        return self.model.predict(model_input).tolist()
    

    
# log_reg_model_name = f'models:/cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.lightgbm/5'
# signature = infer_signature(application_train.drop('TARGET', axis=1), application_train['TARGET'])

lgbm_model_name = f'cat_ah01_dna_d_dlp_gen.ah_starbase_assessment.lightgbm_inference'
application_train = pd.read_csv('/Volumes/cat_ah01_dna_d_dlp_gen/ah_starbase_assessment/data/application_train.csv')

# Log the model with MLflow
with mlflow.start_run(run_name='lightgbm_custom_inference') as run:
    mlflow.pyfunc.log_model(
        registered_model_name=lgbm_model_name,
        artifact_path="lightgbm_custom",
        python_model=CustomModel(model,loaded_le_flag_own_car,loaded_le_flag_own_realty,loaded_le_name_contract_type,features_list,imputer,scaler),
        # signature=signature,
        input_example=application_train.drop('TARGET', axis=1).head(10).reset_index(drop=True).to_dict(orient='records'),
        conda_env={
            'channels': ['defaults'],
            'dependencies': [
                'python=3.8.5',
                'pip',
                {
                    'pip': [
                        'mlflow',
                        'numpy',
                        'pandas',
                        'scikit-learn',
                        'lightgbm'
                    ],
                },
            ],
        },
    )


import os
import requests
import numpy as np
import pandas as pd
import json

def create_tf_serving_json(data):
    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}

def score_model(dataset):
    url = 'https://adb-6515920204052091.11.azuredatabricks.net/serving-endpoints/custom_model/invocations'
    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)
    data_json = json.dumps(ds_dict, allow_nan=True)
    response = requests.request(method='POST', headers=headers, url=url, data=data_json)
    if response.status_code != 200:
        raise Exception(f'Request failed with status {response.status_code}, {response.text}')
    return response.json()

# Example usage
# application_test = pd.read_csv('/Volumes/cat_ah01_dna_d_dlp_gen/ah_starbase_assessment/data/application_test.csv').head(1)
predictions = score_model(application_test.head(1))
predictions




### **Databricks Feature Store: Overview**

The **Databricks Feature Store** is a centralized repository for storing, sharing, and managing machine learning features. It is tightly integrated with Databricks workflows, including Delta Lake, MLflow, and Spark, and is designed to promote **feature reuse, reproducibility, and governance** in ML pipelines.

A feature store helps solve common challenges in machine learning, such as:
- Feature duplication across models.
- Lack of consistency between training and serving features.
- Difficulties tracking feature lineage and governance.

---

### **Key Features of Databricks Feature Store**

#### **1. Centralized Feature Repository**
- Provides a **centralized location** to store and manage features used across multiple ML models.
- Features can be stored as **feature tables**, which are versioned and organized in a Delta Lake-backed structure.

#### **2. Consistent Feature Engineering**
- Ensures **consistent feature computation** across training and inference pipelines.
- Allows developers to define features once and reuse them in multiple ML workflows.

#### **3. Online and Offline Feature Serving**
- **Offline Serving**: Features can be read directly during batch training or inference jobs.
- **Online Serving**: Low-latency feature retrieval via integration with online databases (e.g., AWS DynamoDB or Azure Cosmos DB) for real-time inference.

#### **4. Native Integration with Databricks Ecosystem**
- Fully integrated with **Delta Lake** for versioned feature storage and reliability.
- Works seamlessly with **MLflow**, enabling feature metadata tracking and linking features to registered models.

#### **5. Feature Lineage and Governance**
- Tracks feature **lineage** from the source data to feature generation and usage in models.
- Helps in understanding how features were derived and ensures compliance with governance requirements.

#### **6. Automatic Feature Versioning**
- Features are versioned automatically whenever updates are made, ensuring reproducibility in ML experiments.
- Allows rollbacks to previous versions of features if needed.

#### **7. Sharing and Reuse**
- Facilitates **feature reuse** across teams and projects, reducing redundancy and development time.
- Features can be accessed by multiple models, enabling collaboration.

#### **8. Real-Time Feature Engineering**
- Supports the creation of real-time features using event-based architectures and streaming data.

#### **9. Model Training and Scoring Integration**
- Automatically joins features with labels during model training.
- Provides APIs for scoring models by retrieving features from the feature store.

#### **10. Monitoring and Drift Detection**
- Provides tools to monitor feature usage and detect **feature drift** over time, ensuring model reliability.

---

### **Core Components of Databricks Feature Store**

1. **Feature Table**
   - A structured table that contains features, their metadata, and lineage.
   - Stored as Delta tables in Unity Catalog or a Databricks-managed metastore.
   - Example: A feature table for customer behavior:
     ```plaintext
     customer_id | avg_purchase_amount | total_orders | last_purchase_date
     ```

2. **Feature Store APIs**
   - Python and Scala APIs for creating, managing, and retrieving features.
   - Example:
     ```python
     from databricks.feature_store import FeatureStoreClient
     fs = FeatureStoreClient()

     # Create a feature table
     fs.create_table(
         name="main_catalog.customer_features",
         primary_keys=["customer_id"],
         schema=feature_schema,
         description="Features for customer segmentation"
     )
     ```

3. **Feature Serving Infrastructure**
   - Connects feature tables to training pipelines, batch scoring, and real-time serving systems.
   - Online serving supports low-latency use cases.

---

### **Steps to Use the Databricks Feature Store**

#### **1. Create a Feature Table**
- Define features using a Spark DataFrame or Delta Table and save them in the feature store.

```python
from databricks.feature_store import FeatureStoreClient

fs = FeatureStoreClient()

# Define features
customer_features = spark.sql("""
SELECT
    customer_id,
    AVG(purchase_amount) AS avg_purchase_amount,
    COUNT(order_id) AS total_orders
FROM
    sales_data
GROUP BY
    customer_id
""")

# Create or update the feature table
fs.create_table(
    name="main_catalog.sales.customer_features",
    primary_keys=["customer_id"],
    df=customer_features,
    description="Customer behavioral features"
)
```

#### **2. Use Features for Training**
- Retrieve features from the feature store and join them with labels for training.

```python
# Retrieve features
features = fs.read_table("main_catalog.sales.customer_features")

# Join features with labels
training_data = features.join(labels, on="customer_id")

# Train the model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier().fit(training_data[features], training_data["label"])
```

#### **3. Serve Features for Inference**
- Retrieve features in real-time or batch mode for inference.

**Batch Inference:**
```python
scored_data = fs.score_batch(
    model_uri="runs:/<run-id>/model",
    feature_table="main_catalog.sales.customer_features",
    result_table="main_catalog.sales.scored_data"
)
```

**Real-Time Inference:**
- Configure the feature store to serve features online using DynamoDB, CosmosDB, or other online stores.

---

### **Key Benefits of Databricks Feature Store**

1. **Reproducibility**:
   - Versioning ensures the same features are used during training and serving.

2. **Collaboration**:
   - Encourages feature sharing across teams, reducing development time and duplication.

3. **Performance**:
   - Efficient retrieval of features for both offline and online use cases.

4. **Scalability**:
   - Built on Delta Lake, enabling it to handle large-scale datasets and real-time data.

---

### **Limitations of Databricks Feature Store**

1. **Real-Time Serving Dependencies**:
   - Requires integration with external online storage systems like DynamoDB for real-time use cases.

2. **Cloud Dependency**:
   - Heavily dependent on Databricks and cloud infrastructure.

3. **Limited Non-Spark Integration**:
   - Designed for Spark-based environments, which may be restrictive for teams using other tools.

4. **Complexity for Small Workflows**:
   - Setting up a feature store may be overkill for small or simple ML projects.

---

### **Use Cases for Databricks Feature Store**

1. **Customer Personalization**:
   - Reuse behavioral features like purchase history, website visits, and interactions across multiple personalization models.

2. **Fraud Detection**:
   - Create real-time features based on transaction patterns and reuse them in fraud detection models.

3. **Predictive Maintenance**:
   - Store historical sensor data and derived features for equipment failure prediction models.

4. **Recommendation Systems**:
   - Build and reuse features for collaborative filtering and content-based recommendation engines.

---

If you'd like more details on any specific use case or example, feel free to ask!
