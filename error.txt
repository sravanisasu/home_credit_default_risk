# Generate a PAT with dbutils
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

# Print or store the token securely
print("Generated PAT:")
print(token)


DVC:
(Data Versioning and Reproducible ML with DVC and MLflow) https://www.youtube.com/watch?v=W2DvpCYw22o
(DVC Data Version Control Architecture Overview | #mlops #dvc #machinelearning) https://www.youtube.com/watch?v=NMcKkIEvpHA
 
Monitoring:
(Learn How to Reliably Monitor Your Data and Model Quality in the Lakehouse) https://www.youtube.com/watch?v=3TLBZSKeYTk
 

### **Unity Catalog in Databricks: Overview**

**Unity Catalog** is a unified governance solution provided by Databricks that simplifies data governance and security in a multi-cloud environment. It offers centralized management for **data discovery, access controls, lineage tracking, and audit logging** across Databricks workspaces.

It acts as a **metadata layer** that integrates with existing cloud storage systems (e.g., AWS S3, Azure Data Lake, Google Cloud Storage) to manage data, schemas, and tables.

---

### **Key Features of Unity Catalog**

1. **Centralized Data Governance**
   - Provides a unified platform to manage access to all data assets (files, tables, views).
   - Supports **fine-grained access control** at the table, column, and row levels.

2. **Data Lineage**
   - Tracks the **end-to-end lineage** of data from raw sources to analytics and reports.
   - Lineage includes transformations, queries, and downstream usage.
   - Helps with debugging, auditing, and understanding the lifecycle of data.

3. **Fine-Grained Access Control**
   - Uses **Attribute-Based Access Control (ABAC)** or **Role-Based Access Control (RBAC)** for granting and restricting access.
   - Supports dynamic masking for sensitive columns (e.g., SSNs, credit card numbers).

4. **Multi-Cloud Support**
   - Seamlessly integrates with Databricks workspaces across multiple cloud platforms (AWS, Azure, GCP).

5. **Simplified Data Sharing**
   - **Delta Sharing**: Securely share live data (Delta Lake tables) across organizations without replication.
   - Promotes collaboration without moving data.

6. **Compliance and Audit**
   - Maintains a **full audit trail** of user activity, lineage, and changes to access policies.
   - Helps organizations comply with regulations like GDPR, HIPAA, etc.

7. **Integration with Existing Tools**
   - Works with modern data catalogs and BI tools like Tableau, Power BI, Looker, and others.
   - Integrates with Delta Lake to track data changes efficiently.

8. **Real-Time Metadata Discovery**
   - Allows users to search and discover data assets across the organization using the Databricks UI or APIs.

9. **Schema Enforcement**
   - Prevents schema drift by enforcing column-level definitions and constraints.

---

### **Steps to Enable and Use Lineage in Unity Catalog**

#### **1. Prerequisites**
   - Ensure Unity Catalog is enabled in your Databricks account.
   - You need **Metastore Admin** permissions to set up the Unity Catalog.
   - Use Databricks Runtime **12.0 or higher** to capture data lineage.

#### **2. Create a Unity Catalog Metastore**

   - Go to the **Databricks Admin Console**.
   - Under "Unity Catalog," create a **Metastore** and assign a cloud storage bucket for metadata.

#### **3. Enable Lineage Tracking**

   - Lineage is enabled by default when using Unity Catalog in supported runtimes (Databricks Runtime 12.0+).
   - It automatically tracks lineage for:
     - Queries
     - Delta Lake operations
     - Data loading jobs (e.g., with Spark or Auto Loader).

#### **4. Capture Lineage Using SQL Commands**

   - Unity Catalog automatically captures lineage when users perform **SQL queries** or **Spark DataFrame transformations** on Unity Catalog-managed data.

   Example SQL query:
   ```sql
   USE CATALOG main_catalog;
   CREATE TABLE sales_data AS
   SELECT * FROM raw_data WHERE region = 'US';
   ```
   - Lineage will capture:
     - Input dataset: `raw_data`
     - Output dataset: `sales_data`
     - Transformation: `region = 'US'`

#### **5. View Lineage Information**

   - Open the **Unity Catalog Lineage View** in the Databricks UI:
     - Go to **Data Explorer**.
     - Select a table or view.
     - Click the **Lineage** tab to view upstream and downstream dependencies.

#### **6. Programmatically Access Lineage**

   Use Databricks REST APIs or CLI to retrieve lineage data programmatically.

   Example API call:
   ```bash
   curl -X GET \
   -H "Authorization: Bearer <your-token>" \
   https://<databricks-instance>/api/2.0/lineage/table?table=<table-name>
   ```

---

### **Lineage Example Use Case**

#### Data Pipeline:
   - **Input Data**: Raw files in S3 or Delta Lake tables.
   - **Transformations**:
     - Aggregate and filter data using Spark/SQL.
     - Write results to a `sales_summary` table in Unity Catalog.
   - **Data Consumers**:
     - BI tools like Tableau query the `sales_summary` table.

#### Lineage Insights:
   - **Upstream Data**: Raw files and tables (`raw_data`).
   - **Transformations**: SQL queries, filtering, aggregation.
   - **Downstream Consumers**: `sales_summary` table used by Tableau.

   Lineage graph shows the entire flow from raw files to analytics, helping with audits and debugging.

---

### **Key Benefits of Lineage in Unity Catalog**

1. **Auditability**: Easily track changes to data, transformations, and metadata.
2. **Debugging**: Quickly identify bottlenecks or errors in data pipelines.
3. **Impact Analysis**: Assess how changes to upstream datasets affect downstream applications.
4. **Compliance**: Demonstrates data usage and lineage to meet regulatory requirements.

---

### **Limitations of Lineage in Unity Catalog**

1. **Runtime Dependency**: Requires Databricks Runtime 12.0+ for lineage tracking.
2. **Non-Databricks Workflows**: Does not capture lineage for workflows running outside Databricks (e.g., external ETL pipelines).
3. **Lineage Completeness**: Some manual processes or undocumented transformations might not be captured.

---

If you need specific examples, such as SQL commands, API integration, or how to manage policies, let me know!
